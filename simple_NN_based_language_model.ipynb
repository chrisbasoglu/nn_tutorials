{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "57b43a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ef7aba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make the same LM as we did with the Counting base, but use NN instead.\n",
    "\n",
    "# Instead of counting all the bi-gram \"character pairs\" in a 2D array and then normalizing each row to get \n",
    "# the probability distribution for any particular character following another particular character.  \n",
    "\n",
    "# our neural network will receive a single character as an input, and then the neural network will output the \n",
    "# probability distribution of all possible next characters.  So we will have 27 outputs, each output standing in for one possible\n",
    "# next character, and each output being the probability that that particular character is next after the input.\n",
    "# \n",
    "# We will use the data set which has the input characters and output characters (next character from the input) as the training\n",
    "# set and will train the weights, using the loss function we created (negative log likelihood) that we will minimize during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "68dc2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file with list of\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "09661e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "24105f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are len(words)=32033 names in this dataset\n"
     ]
    }
   ],
   "source": [
    "# Count number of names in the dataset\n",
    "print(f\"There are {len(words)=} names in this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "625e1dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min word length is: 2\n",
      "max word length is: 15\n"
     ]
    }
   ],
   "source": [
    "# find shortest name and longest name.\n",
    "# (len(w) for w in words) creates an iterator across all the word lengths.\n",
    "\n",
    "# min and max take iterators as input, so:\n",
    "print(f\"min word length is: {min(len(w) for w in words)}\")\n",
    "print(f\"max word length is: {max(len(w) for w in words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "22f021ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Put all characters into our dataset into one string, with ''.join(words), remove all duplicate characters with set, convert to list,\n",
    "# then sort.\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "print(f\"{chars=}\")\n",
    "\n",
    "# Dictionary to map from char to integer index\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "# Dictionary to map from integer index to char\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "69a4a25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e  =>  0, 5\n",
      "e m  =>  5, 13\n",
      "m m  =>  13, 13\n",
      "m a  =>  13, 1\n",
      "a .  =>  1, 0\n",
      "xs=tensor([ 0,  5, 13, 13,  1])\n",
      "ys=tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "\n",
    "# inputs\n",
    "xs = []\n",
    "\n",
    "# outputs (targets)\n",
    "ys = []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(f\"{ch1} {ch2}  =>  {ix1}, {ix2}\")\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f\"{xs=}\")\n",
    "print(f\"{ys=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d487a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have the input values which are single integer indices.  We can't really apply weights to a single input integer index.  \n",
    "# We need a vector of some kind to we end up with more than one node in the input layer.  The correct way to handle this is \n",
    "# \"one-hot encoding\".  If the max number is 10 for example. then one hot of 5 is [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] and\n",
    "# ine hot of 3 is [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# take all the xs and create one-hot. Each element can range from 0 to 26 (27 total). In our xs, \n",
    "# we have 5 integer elements: xs=tensor([ 0,  5, 13, 13,  1]).  For each of the 5, create a one-hot based on its value.\n",
    "# That first element is 0, so the one hot would look like [1, 0, 0, 0, 0, 0, 0, ...], the second element is 5 so the \n",
    "# one hot would look like [0, 0, 0, 0, 0, 1, 0, ...]\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ca2e03d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9501d556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets define our NN.  We have 27-element vectors (one hot) as inputs.  And the final output will have 27 outputs.  \n",
    "# If we define a single layer network, the simplest kind, we need 27 nodes.  That is the weights for this single layer\n",
    "# is 27x27.  Start with random weights.\n",
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g) \n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ea68a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the weights start out as random numbers, each individual weight starts out as random number plucked from a normal\n",
    "# distributution as float number where center is 0.0 but can be positive of negative, the larger the absolute value, \n",
    "# the rarer, but can be a large as possible.\n",
    "\n",
    "# When the inputs (in this case one-hots), get multiplied by the random wights, the 27 outputs will end up being positive or \n",
    "# negative numbers whose absolute value can be from 0 to large.  What we want however is probabilities.  One way to think\n",
    "# of things is that the outputs are log(counts). Lets define the term logits to define log(counts). With log of counts, they \n",
    "# can be positive or negative with absolute value both below and above one.  To get counts, then, we'd do exponent.  \n",
    "# So counts = exp(out) or  counts = exp(logits)\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "\n",
    "# Forward pass of our 1 layer NN (Note @ is matrix multiplication)\n",
    "logits = xenc @ W # predict log-counts\n",
    "\n",
    "# Treat the outputs of our NN as log(counts) or logits.  To get the counts, do exp().  Thats equivalent to our 2D array where we counted\n",
    "# the occurances of each character pair N in our simple counting based example before. \n",
    "counts = logits.exp() \n",
    "\n",
    "# Now convert to row probability by dividing each row by row sim.\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "265c0afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax.jpeg\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BTW: the last 2 lines here are together called a 'softmax' and are the standard way to generate probabilities.\n",
    "Image(url=\"softmax.jpeg\", width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "625d3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "the 27 output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label or target (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "the 27 output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label or target (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "the 27 output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label or target (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "the 27 output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label or target (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "the 27 output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label or target (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "# Lets go through the Forward each of the 5 inputs from the first word.\n",
    "nlls = []\n",
    "\n",
    "# i idexts through each of the 5 inputs (and corresponding target) from the first word\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('the 27 output probabilities from the neural net:', probs[i])\n",
    "  print('label or target (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls.append(nll)\n",
    "\n",
    "print('=========')\n",
    "nlls = torch.tensor(nlls)\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fd625ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs = []\n",
    "ys = []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network' with random\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "63b6399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0: loss=3.758953332901001\n",
      "iteration 10: loss=2.6890029907226562\n",
      "iteration 20: loss=2.572789192199707\n",
      "iteration 30: loss=2.5301806926727295\n",
      "iteration 40: loss=2.5086867809295654\n",
      "iteration 50: loss=2.496137857437134\n",
      "iteration 60: loss=2.488049268722534\n",
      "iteration 70: loss=2.482424259185791\n",
      "iteration 80: loss=2.478290319442749\n",
      "iteration 90: loss=2.475132703781128\n",
      "------------------------------------\n",
      "final loss:        2.4728763103485107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "\n",
    "  # Each row in the prob is from the next input, so for the first 5 inputs, we want row 0, row 1, row 2, etc.\n",
    "  # torch.arange(num) would give [0, 1, 2, 3, 4] so to get each row, would be  probs[torch.arange(num)].  \n",
    "  # For each row, we want to pluck out the index that corresponds to the correct target which is the current\n",
    "  # probability computed for that one out of 27 target result.  So to get those we have\n",
    "  # probs[torch.arange(num), ys].  Now we then want to get to average negative log probability as the loss, so\n",
    "  # end up with:\n",
    "  loss = -1.0 * probs[torch.arange(num), ys].log().mean()\n",
    "  \n",
    "  if (k%10 == 0):\n",
    "     print(f\"iteration {k:2}: loss={loss.item()}\")\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(f\"final loss:        {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1d4f77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, that remember in hte previous counting example, we did model smoothing on the counts, to prevent counts of zero that lead\n",
    "# to probabilities of 0 and infinity in later math.  In order to do the smoothing we just added a number like 1 to each count.  To smooth\n",
    "# more we;d add a larger number.  If we add a really large number, we'd have uniform distribution across the row, and each probability the\n",
    "# same and no prediction power at all in the model.  We'd get uniformly random outputs from our inputs.  Uniform distribution is equivalent\n",
    "# to all the weights being 0, since the exp in the softmax would take all the zero outputs (due to all W being 0), exponentiate to all 1's and\n",
    "# then dividing by row, sum ending in uniform distribution per row.  So the a small trend towards weights of 0 is like model normalization.\n",
    "# so we could add a term to the loss function that when minimized trends the elements of W towards zero, but we only want a small amount of this.\n",
    "# If we sum the square of each W element, or mean the square of each element, then as as a loss, and loss is minimized, the W will trend toward \n",
    "# zero.  To get only a little of this, we can multiply that term bu 0.01. And thus we get the new loss of below.  We call this regularization.\n",
    "\n",
    "loss = (-1.0 * probs[torch.arange(num), ys].log().mean()) + 0.01*(W**2).mean()\n",
    "\n",
    "# take the new loss and replace this with the older loss in the code and rerun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e5f47b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mor.\n",
      "axwaninaymoryles.\n",
      "kondmaisah.\n",
      "anchshizarie.\n",
      "odaren.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0d8714401c84b2f34c97cdbc2dc9b27c772eb6be171c58b92b0887b6275033e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
